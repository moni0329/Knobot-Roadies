{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c0b93358-e81a-4a42-84c9-a9b8015b54de",
   "metadata": {},
   "source": [
    "test 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a1210e-ac7a-4675-922f-ebd39ea65473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 113.6ms pre-process, 678.8ms inference, 13.2ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp27\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[415.35855, 317.60614, 487.28140, 360.33487,   0.89834,   0.00000]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/tissascl.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6898e2a2-f72b-48d5-832b-dcab8fc40af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 150x150 (no detections)\n",
      "Speed: 36.9ms pre-process, 1075.4ms inference, 8.1ms NMS per image at shape (1, 3, 640, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp25\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([], size=(0, 6))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/town_no.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3848716-6f53-43d1-89f7-37818a1cfe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 866.9ms pre-process, 755.5ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp21\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([], size=(0, 6))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/y_no.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806709de-770a-4219-86f6-9f18d1306e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 142.3ms pre-process, 774.0ms inference, 3.6ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp24\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[464.64108, 312.82608, 580.85376, 403.81985,   0.88844,   0.00000]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/pure.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2302077b-5cf4-4ec6-ad49-43a2cb639865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1511.0ms pre-process, 553.7ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp12\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([], size=(0, 6))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/c_yes.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b435e6-24ab-4cf1-b3ae-0d57777f2c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 632x1116 2 crosswalks\n",
      "Speed: 495.3ms pre-process, 524.9ms inference, 4.5ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp28\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[5.79513e+02, 3.30664e+02, 6.72546e+02, 3.99270e+02, 7.85187e-01, 0.00000e+00],\n",
      "        [9.66628e+02, 6.10823e+02, 1.01605e+03, 6.27287e+02, 2.54272e-01, 0.00000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to your YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/t_yes.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e4a8d9f-6bf8-4311-b5ba-8004490012c3",
   "metadata": {},
   "source": [
    "test images set in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907e451a-df42-44d2-950f-3b08b4cf7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 687.6ms pre-process, 427.5ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: a_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 632x1116 (no detections)\n",
      "Speed: 219.4ms pre-process, 578.3ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: bus_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 696.6ms pre-process, 441.4ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: b_yes.png\n",
      "Image Name: c_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 860.5ms pre-process, 620.5ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 983.1ms pre-process, 633.5ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: d_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 824.7ms pre-process, 635.8ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: e_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 864.7ms pre-process, 514.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: g_no.png\n",
      "Image Name: h_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 977.9ms pre-process, 847.7ms inference, 8.1ms NMS per image at shape (1, 3, 384, 640)\n",
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 977.4ms pre-process, 730.4ms inference, 3.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: i_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 1080x1920 (no detections)\n",
      "Speed: 271.4ms pre-process, 676.5ms inference, 1.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: jungle.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 942.4ms pre-process, 626.8ms inference, 3.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: j_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1016.2ms pre-process, 1053.7ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: k_no.png\n",
      "Image Name: l_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1041.9ms pre-process, 829.4ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: mihintalea_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1155.6ms pre-process, 820.1ms inference, 1.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1049.7ms pre-process, 644.1ms inference, 1.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: mihintaleb_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 1073.5ms pre-process, 788.0ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: m_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 941.4ms pre-process, 637.0ms inference, 1.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: n_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 913.4ms pre-process, 627.9ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: o_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 102.3ms pre-process, 776.3ms inference, 4.4ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: pure.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 1209.4ms pre-process, 1006.0ms inference, 2.6ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: p_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ss\\AppData\\Local\\Temp\\ipykernel_3232\\672707550.py:23: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 112.3ms pre-process, 726.9ms inference, 3.7ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: tissascl.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 150x150 (no detections)\n",
      "Speed: 34.0ms pre-process, 1240.0ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: town_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 632x1116 2 crosswalks\n",
      "Speed: 122.6ms pre-process, 692.0ms inference, 2.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: t_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1122.5ms pre-process, 652.9ms inference, 1.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: y_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1043.8ms pre-process, 706.9ms inference, 1.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: z_no.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the path to your YOLOv5 repository and the trained model weights\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "model_weights = 'C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path=model_weights, source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the directory containing images for testing\n",
    "image_dir = Path('E:/testing')\n",
    "\n",
    "# Function to plot images in Jupyter Notebook\n",
    "def plot_image_with_boxes(img, results, img_name):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    ax.imshow(img)\n",
    "    for *box, conf, cls in results.xyxy[0]:  # xyxy format\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "        ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none', lw=2))\n",
    "        ax.text(x1, y1, label, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.title(img_name)  # Set the title as the image name\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over images in the directory\n",
    "for img_path in image_dir.glob('*.png'):  # Adjust the file extension if necessary\n",
    "    img = Image.open(img_path)\n",
    "    results = model(img)\n",
    "\n",
    "    # Convert the image to numpy array for displaying\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Print the image name\n",
    "    print(f'Image Name: {img_path.name}')\n",
    "\n",
    "    # Plot the image with bounding boxes and print the image name\n",
    "    plot_image_with_boxes(img_np, results, img_path.name)\n",
    "\n",
    "    # Optionally print results\n",
    "    results.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b09c83-fdfc-485f-a9bf-4ecb82f00eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
