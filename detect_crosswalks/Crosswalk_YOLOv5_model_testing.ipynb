{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ee023a99-f839-40d3-b038-f2d4d9f2c305",
   "metadata": {},
   "source": [
    "test 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a1210e-ac7a-4675-922f-ebd39ea65473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 262.3ms pre-process, 468.5ms inference, 15.6ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp29\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[415.35855, 317.60614, 487.28140, 360.33487,   0.89834,   0.00000]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the trained YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/tissascl.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6898e2a2-f72b-48d5-832b-dcab8fc40af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 150x150 (no detections)\n",
      "Speed: 143.5ms pre-process, 784.4ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp30\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([], size=(0, 6))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the trained YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/town_no.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3848716-6f53-43d1-89f7-37818a1cfe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1215.3ms pre-process, 817.3ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp31\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([], size=(0, 6))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the trained YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/y_no.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806709de-770a-4219-86f6-9f18d1306e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 274.4ms pre-process, 637.2ms inference, 15.6ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp32\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[464.64108, 312.82608, 580.85376, 403.81985,   0.88844,   0.00000]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the trained YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/pure.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2302077b-5cf4-4ec6-ad49-43a2cb639865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1188.6ms pre-process, 795.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp33\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([], size=(0, 6))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the trained YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/c_yes.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b435e6-24ab-4cf1-b3ae-0d57777f2c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 632x1116 2 crosswalks\n",
      "Speed: 144.8ms pre-process, 433.1ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "Saved 1 image to \u001b[1mruns\\detect\\exp34\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[5.79513e+02, 3.30664e+02, 6.72546e+02, 3.99270e+02, 7.85187e-01, 0.00000e+00],\n",
      "        [9.66628e+02, 6.10823e+02, 1.01605e+03, 6.27287e+02, 2.54272e-01, 0.00000e+00]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Path to YOLOv5 repository\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "\n",
    "# Load the trained YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path='C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt', source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example of using the model for inference\n",
    "img = 'E:/testing/t_yes.png'\n",
    "\n",
    "# Perform inference\n",
    "results = model(img)\n",
    "\n",
    "# Print results\n",
    "results.print()  # Print results to the console\n",
    "\n",
    "# Save results to an image file\n",
    "results.save('E:/testing/results')  # Save the results image to the specified path\n",
    "\n",
    "# To access the bounding boxes and other details\n",
    "print(results.xyxy)  # Bounding box coordinates and scores\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e4a8d9f-6bf8-4311-b5ba-8004490012c3",
   "metadata": {},
   "source": [
    "test images set in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "907e451a-df42-44d2-950f-3b08b4cf7482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-331-gab364c98 Python-3.11.9 torch-2.3.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 722.4ms pre-process, 506.4ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: a_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 632x1116 (no detections)\n",
      "Speed: 98.5ms pre-process, 456.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: bus_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 746.7ms pre-process, 496.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: b_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 723.9ms pre-process, 421.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: c_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 1065.3ms pre-process, 593.7ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: d_yes.png\n",
      "Image Name: e_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1012.5ms pre-process, 671.8ms inference, 15.6ms NMS per image at shape (1, 3, 384, 640)\n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 895.5ms pre-process, 593.7ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: g_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 753.5ms pre-process, 476.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: h_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 897.4ms pre-process, 453.3ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: i_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 1080x1920 (no detections)\n",
      "Speed: 187.5ms pre-process, 438.6ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: jungle.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 760.0ms pre-process, 478.7ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: j_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 752.7ms pre-process, 435.7ms inference, 15.6ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: k_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 720.9ms pre-process, 468.7ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: l_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 652.1ms pre-process, 546.5ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: mihintalea_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1192.4ms pre-process, 687.5ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: mihintaleb_yes.png\n",
      "Image Name: m_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 1026.9ms pre-process, 639.8ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n",
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 840.9ms pre-process, 656.2ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: n_no.png\n",
      "Image Name: o_yes.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 2 crosswalks\n",
      "Speed: 745.1ms pre-process, 720.9ms inference, 15.6ms NMS per image at shape (1, 3, 384, 640)\n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 125.0ms pre-process, 724.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: pure.png\n",
      "Image Name: p_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 1 crosswalk\n",
      "Speed: 1091.9ms pre-process, 1020.7ms inference, 15.6ms NMS per image at shape (1, 3, 384, 640)\n",
      "C:\\Users\\ss\\AppData\\Local\\Temp\\ipykernel_15684\\847790312.py:23: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
      "image 1/1: 632x1116 1 crosswalk\n",
      "Speed: 132.2ms pre-process, 1116.4ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: tissascl.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 150x150 (no detections)\n",
      "Speed: 15.6ms pre-process, 1245.9ms inference, 0.0ms NMS per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: town_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 632x1116 2 crosswalks\n",
      "Speed: 123.3ms pre-process, 797.4ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: t_yes.png\n",
      "Image Name: y_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1150.9ms pre-process, 992.8ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Name: z_no.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1: 2160x3840 (no detections)\n",
      "Speed: 1212.2ms pre-process, 841.9ms inference, 0.0ms NMS per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the path to YOLOv5 repository and the trained model weights\n",
    "repo_path = 'C:/Users/ss/yolov5/yolov5'\n",
    "model_weights = 'C:/Users/ss/yolov5/yolov5/runs/train/exp6/weights/best.pt'\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model = torch.hub.load(repo_path, 'custom', path=model_weights, source='local')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the directory containing images for testing\n",
    "image_dir = Path('E:/testing')\n",
    "\n",
    "# Function to plot images in Jupyter Notebook\n",
    "def plot_image_with_boxes(img, results, img_name):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "    ax.imshow(img)\n",
    "    for *box, conf, cls in results.xyxy[0]:  # xyxy format\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "        ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none', lw=2))\n",
    "        ax.text(x1, y1, label, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.title(img_name)  # Set the title as the image name\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over images in the directory\n",
    "for img_path in image_dir.glob('*.png'):  # We can adjust the file extension if we need\n",
    "    img = Image.open(img_path)\n",
    "    results = model(img)\n",
    "\n",
    "    # Convert the image to numpy array for displaying\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Print the image name\n",
    "    print(f'Image Name: {img_path.name}')\n",
    "\n",
    "    # Plot the image with bounding boxes and print the image name\n",
    "    plot_image_with_boxes(img_np, results, img_path.name)\n",
    "\n",
    "    # Optionally print results\n",
    "    results.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b09c83-fdfc-485f-a9bf-4ecb82f00eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
